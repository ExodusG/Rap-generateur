{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projet IA - Rap generation\n",
        "# Quentin Le Lan & Marius Le Douarin\n",
        "\n",
        "\n",
        "This project aims to train a GPT-2 model to generate French rap for us. To achieve this, we followed the advice provided [here](https://discuss.huggingface.co/t/fine-tune-gpt2-for-french-belgium-rap/7098). You can find in this notebook instructions on how to train the model, how to use it, and a graphical interface.\n",
        "\n",
        "We use the model **louis2020belgpt2** from [github](https://github.com/antoiloui/belgpt2) or [huggingface](https://huggingface.co/antoinelouis/belgpt2)\n",
        "\n",
        "author = Louis, Antoine\n",
        "title = BelGPT-2: a GPT-2 model pre-trained on French corpora.\n",
        "year = 2020\n",
        "\n"
      ],
      "metadata": {
        "id": "_EJ4IevI1DIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxlib==0.4.20 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install transformers datasets flax\n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "ibIBco_411b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e05edfd-b580-428b-8b20-456912d5c63d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jaxlib==0.4.20\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.20%2Bcuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl (138.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.4.20) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.4.20) (1.23.5)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.4.20) (0.2.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.23+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.23+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.23+cuda12.cudnn89\n",
            "Successfully installed jaxlib-0.4.20+cuda12.cudnn89\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.23)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.8)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (1.11.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.7)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.4.20+cuda12.cudnn89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the GPT model"
      ],
      "metadata": {
        "id": "qDRCFx2j1VL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the command to fine tune the model a first time. You need the file [flax](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_clm_flax.py). The time of an epoch depends of the size of your dataset. For and dataset of 22000 it's take ~1h"
      ],
      "metadata": {
        "id": "p_PZ1KoV7bPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hGbb4VcibhO"
      },
      "outputs": [],
      "source": [
        "!python run_clm_flax.py --model_name_or_path antoiloui/belgpt2 --train_file train.csv --do_eval --validation_file validation.csv --output_dir output --do_train --preprocessing_num_workers 2 --num_train_epoch 1 --block_size=1024 --per_device_train_batch_size 4 --eval_steps 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the command for fine tune the model after the first epoch. You should be becareful to the param `config_name`, `tokenize_name`, `model_name_or_path`"
      ],
      "metadata": {
        "id": "v4kP2lhp_aUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFTybBbaDADX"
      },
      "outputs": [],
      "source": [
        "!python run_clm_flax.py --config_name ./drive/MyDrive/ia/epoch4/config.json --tokenizer_name ./drive/MyDrive/ia/epoch4/ --model_name_or_path ./drive/MyDrive/ia/epoch4/flax_model.msgpack --train_file ./drive/MyDrive/ia/train.csv --do_eval --validation_file ./drive/MyDrive/ia/validation.csv --output_dir ./drive/MyDrive/ia/epoch5 --do_train --preprocessing_num_workers 2 --num_train_epoch 1 --block_size=1024 --per_device_train_batch_size 3 --eval_steps 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the model localy\n",
        "\n",
        "It's to generate with no interface"
      ],
      "metadata": {
        "id": "Bo5sraJl1eGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DGG6WpKIV5tx"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForCausalLM,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 40 #@param {type:\"slider\", min:1, max:500, step:10}\n",
        "MIN_LEN = 31 #@param {type:\"slider\", min:1, max:500, step:10}\n",
        "temp = 0.7 #@param {type:\"slider\", min:0.0, max:2.0, step:0.1}\n",
        "top_p=0.95 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "top_k=100 #@param {type:\"slider\", min:0, max:1000, step:10}\n",
        "repetition_penalty=1.5 #@param {type:\"slider\", min:0.0, max:10, step:0.5}\n",
        "input_text=\"Négro jtire une taffe, jfais des gros nuages Jrappe tellement ma life, ça devient meme plus un jeu\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "Dhm4z1b54bXL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use it, change the path of the `config`, `tokenizer` and the `model`"
      ],
      "metadata": {
        "id": "TgEe3GAKv7nx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pywbnCsbNtC",
        "outputId": "6a5a1f6b-6978-42ba-d44d-112cbb92e854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Négro jtire une taffe, jfais des gros nuages Jrappe tellement ma life, ça devient meme plus un jeu  j'ai la bite d'un mec qui veut\"]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "config = AutoConfig.from_pretrained('./drive/MyDrive/ia/epoch4/config.json')\n",
        "tokenizer = AutoTokenizer.from_pretrained('drive/MyDrive/ia/epoch4')\n",
        "model = FlaxAutoModelForCausalLM.from_pretrained('drive/MyDrive/ia/epoch4/flax_model.msgpack',config=config)\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"np\")\n",
        "attention_mask = np.ones(input_ids.shape)\n",
        "\n",
        "\n",
        "output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id,do_sample=True,\n",
        "            top_k=top_k,\n",
        "            max_length=MAX_LEN,\n",
        "            min_length=MIN_LEN,\n",
        "            top_p=top_p,\n",
        "            temperature=temp,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            num_return_sequences=1)\n",
        "\n",
        "output=np.array(output.sequences)\n",
        "\n",
        "decoded_output = []\n",
        "for sample in output:\n",
        "    decoded_output.append(tokenizer.decode(sample, skip_special_tokens=True))\n",
        "print(decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use graphic interface\n",
        "\n",
        "We use streamlite to make a great interface like chatGPT"
      ],
      "metadata": {
        "id": "ZGyyTeOh16tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "v3nLxXPB2M90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-iogVg_2BsU",
        "outputId": "c13b0017-4645-4307-8011-3b5d5b9c322c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.163s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the app\n",
        "\n",
        "Before this part go to `The streamlite app`"
      ],
      "metadata": {
        "id": "XFKCv-gk2V76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "NeOfLqe82YaG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expose the port 8501**\n",
        "\n",
        "Then just click in the `url` showed.\n",
        "\n",
        "A `log.txt`file will be created. Copy the IP adresse of the \"External URL\" and past it in the url of the localtunnel"
      ],
      "metadata": {
        "id": "pz6jc6tf2jOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpHc6gUa2m3D",
        "outputId": "e9a62c8e-a220-46ae-eaa1-149bd527bb04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.04s\n",
            "your url is: https://public-bears-happen.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The sreamlite app"
      ],
      "metadata": {
        "id": "f63xGEXs22LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use it, change the path of the `config`, `tokenizer` and the `model`"
      ],
      "metadata": {
        "id": "ioGEWWxnx8bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForCausalLM,\n",
        ")\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "config = AutoConfig.from_pretrained('./drive/MyDrive/ia/epoch4/config.json')\n",
        "tokenizer = AutoTokenizer.from_pretrained('drive/MyDrive/ia/epoch4')\n",
        "@st.cache_resource\n",
        "def loadModel():\n",
        "  return FlaxAutoModelForCausalLM.from_pretrained('drive/MyDrive/ia/epoch4/flax_model.msgpack',config=config)\n",
        "model=loadModel()\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "  with st.form(\"slider_form\"):\n",
        "    with st.expander(\"Options\"):\n",
        "      MAX_LEN = st.slider('Max length of the sentence',1, 500, 100, step=10)\n",
        "      MIN_LEN = st.slider('Min length of the sentence',1, 500, 50, step=10)\n",
        "      temp = st.slider('Balance between deterministic outputs and creative exploration',0.0, 2.0, 0.7, step=0.1,help=\"Here high values tend to flatten the distribution of probabilities\")\n",
        "      penalty = st.slider('How strongly should we discourage repetitive tokens',0.0, 10.0, 2.0, step=1.0)\n",
        "      top_k = st.slider('The number of token with highest probability to keep (top-k-filtering)',0, 1000, 100, step=10)\n",
        "      top_p = st.slider('Keeps only tokens whose summed probabilities are greater than or equal to top_p (top-p-sampling)',0.0, 3.0, 0.90, step=0.1)\n",
        "      submit = st.form_submit_button(\"Submit Slider Values\")\n",
        "with col2:\n",
        "  for message in st.session_state.messages:\n",
        "      with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "if prompt := st.chat_input(\"Say something\"):\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) # on écrit le prompt dans l'historique\n",
        "  with col2:\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt) # on écrit le prompt\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      message_placeholder = st.empty()\n",
        "      input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "      attention_mask = np.ones(input_ids.shape)  # Créer un masque d'attention avec des 1 pour tous les tokens\n",
        "\n",
        "      output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id,do_sample=True,\n",
        "          top_k=top_k,\n",
        "          max_length=MAX_LEN,\n",
        "          min_length=MIN_LEN,\n",
        "          top_p=top_p,\n",
        "          temperature=temp,\n",
        "          repetition_penalty=penalty,\n",
        "          num_return_sequences=1)\n",
        "\n",
        "      output=np.array(output.sequences)\n",
        "\n",
        "      decoded_output = []\n",
        "      for sample in output:\n",
        "          tmp=tokenizer.decode(sample, skip_special_tokens=True)\n",
        "          decoded_output.append(tmp)\n",
        "      message_placeholder.markdown( ' '.join(decoded_output))\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\":  ' '.join(decoded_output)})#on la push à l'historique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVMdpDgI248U",
        "outputId": "3ea0f853-4ed6-4eeb-e0b1-375f8a827e1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "qDRCFx2j1VL7",
        "Bo5sraJl1eGS",
        "ZGyyTeOh16tO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}